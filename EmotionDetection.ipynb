{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# https://www.kaggle.com/shrivastava/isears‚Äêdataset\n",
    "label_renamer = {'joy': 'happy', 'sadness': 'sad'}\n",
    "data_isear = pd.read_csv('data/isear.csv', sep='|', engine='python')\n",
    "data_isear['label'] = data_isear['Field1'].map(lambda l: label_renamer.setdefault(l, l))\n",
    "data_isear = pd.concat([data_isear['SIT'], data_isear['label']], axis=1, keys=['text', 'label'])\n",
    "\n",
    "# https://www.site.uottawa.ca/~diana/resources/emotion_stimulus_data/\n",
    "texts = []\n",
    "labels = []\n",
    "path = 'data/Diman et al'\n",
    "for file_name in os.listdir(path):\n",
    "    if file_name != 'Readme.txt':\n",
    "        with open(os.path.join(path, file_name)) as f:\n",
    "            for line in f.readlines():\n",
    "                line = re.sub(r'<cause>.*<\\\\cause>', '', line)\n",
    "                token = line.split('>')\n",
    "                label = token[0][1:]\n",
    "                labels.append(label)\n",
    "                texts.append(token[1][:-(len(label) + 2)])\n",
    "data_diman = pd.concat([pd.Series(texts), pd.Series(labels)], axis=1, keys=['text', 'label'])\n",
    "\n",
    "# https://www.aclweb.org/anthology/I17-1099/\n",
    "texts = []\n",
    "labels = []\n",
    "with open('data/EMNLP_dataset/dialogues_text.txt') as f_text:\n",
    "    with open('data/EMNLP_dataset/dialogues_emotion.txt') as f_label:\n",
    "        label_decoder = {'1': 'anger', '2': 'disgust', '3': 'fear', '4': 'happy', '5': 'sad', '6': 'surprise'}\n",
    "        for line in f_text:\n",
    "            text_tokens = line.strip().split('__eou__')\n",
    "            label_tokens = f_label.readline().strip().split(' ')\n",
    "            for i in range(len(label_tokens)):\n",
    "                if label_tokens[i] != '0':\n",
    "                    texts.append(text_tokens[i])\n",
    "                    labels.append(label_decoder[label_tokens[i]])\n",
    "data_emnlp = pd.concat([pd.Series(texts), pd.Series(labels)], axis=1, keys=['text', 'label'])\n",
    "\n",
    "# http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html\n",
    "texts = []\n",
    "labels = []\n",
    "path = 'data/EmoInt'\n",
    "for file_name in os.listdir(path):\n",
    "    with open(os.path.join(path, file_name)) as f:\n",
    "        for line in f:\n",
    "            tokens = line.split('\\t')\n",
    "            texts.append(re.sub(r'@+\\w+', '', tokens[1]))\n",
    "            labels.append(label_renamer.setdefault(tokens[2], tokens[2]))\n",
    "data_emoint = pd.concat([pd.Series(texts), pd.Series(labels)], axis=1, keys=['text', 'label'])\n",
    "\n",
    "data = pd.concat([data_isear, data_diman, data_emnlp, data_emoint])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johngilbertson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johngilbertson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johngilbertson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def get_tokens(sentence):\n",
    "    # tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in stopwords and len(token) > 1]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    return lemma\n",
    "token_list = data['text'].apply(get_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Encode data for model\n",
    "import keras.preprocessing as pp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare tokenizer\n",
    "t = pp.text.Tokenizer()\n",
    "t.fit_on_texts(token_list)\n",
    "\n",
    "max_len = 60\n",
    "# integer encode the documents\n",
    "encoded_texts = t.texts_to_sequences(data['text'])\n",
    "X = pp.sequence.pad_sequences(encoded_texts, maxlen=max_len, padding='post')\n",
    "le = preprocessing.LabelEncoder()\n",
    "Y = le.fit_transform(data['label'])\n",
    "\n",
    "# now splitting into test and training data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create the embedding matrix for the embedding layer\n",
    "vocab_size = len(t.word_index) + 1\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.{}d.txt'.format(embedding_dim)) as f:\n",
    "    for line in f:\n",
    "        word, *vector = line.split()\n",
    "        if word in t.word_index:\n",
    "            idx = t.word_index[word]\n",
    "            embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 60)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 60, 100)           1815200   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 60, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 60, 100)           20100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 2,007,008\n",
      "Trainable params: 2,007,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "575/575 - 76s - loss: 1.2013 - accuracy: 0.5796 - val_loss: 0.9608 - val_accuracy: 0.6773\n",
      "Epoch 2/10\n",
      "575/575 - 64s - loss: 0.9017 - accuracy: 0.6937 - val_loss: 0.8249 - val_accuracy: 0.7210\n",
      "Epoch 3/10\n",
      "575/575 - 61s - loss: 0.7848 - accuracy: 0.7335 - val_loss: 0.8041 - val_accuracy: 0.7310\n",
      "Epoch 4/10\n",
      "575/575 - 61s - loss: 0.6987 - accuracy: 0.7645 - val_loss: 0.7737 - val_accuracy: 0.7447\n",
      "Epoch 5/10\n",
      "575/575 - 67s - loss: 0.6445 - accuracy: 0.7800 - val_loss: 0.7778 - val_accuracy: 0.7437\n",
      "Epoch 6/10\n",
      "575/575 - 71s - loss: 0.5838 - accuracy: 0.8043 - val_loss: 0.7977 - val_accuracy: 0.7473\n",
      "Epoch 7/10\n",
      "575/575 - 70s - loss: 0.5409 - accuracy: 0.8143 - val_loss: 0.8100 - val_accuracy: 0.7427\n",
      "Epoch 8/10\n",
      "575/575 - 64s - loss: 0.4867 - accuracy: 0.8381 - val_loss: 0.8183 - val_accuracy: 0.7432\n",
      "Epoch 9/10\n",
      "575/575 - 62s - loss: 0.4498 - accuracy: 0.8457 - val_loss: 0.8788 - val_accuracy: 0.7426\n",
      "Epoch 10/10\n",
      "575/575 - 61s - loss: 0.4157 - accuracy: 0.8594 - val_loss: 0.9010 - val_accuracy: 0.7362\n",
      "192/192 - 4s - loss: 0.9231 - accuracy: 0.7344\n",
      "Accuracy: 73.441070\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# build bidirectional LSTM\n",
    "input_layer = Input(shape=(max_len,))\n",
    "model = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(input_layer)\n",
    "model = Bidirectional(LSTM(embedding_dim, return_sequences=True, dropout=0.50), merge_mode='concat')(model)\n",
    "model = TimeDistributed(Dense(embedding_dim, activation='relu'))(model)\n",
    "model = GlobalMaxPool1D()(model)\n",
    "model = Dense(100, activation='relu')(model)\n",
    "output_layer = Dense(Y.max() + 1, activation='softmax', name='prediction')(model)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, Y_train, validation_split=0.25, epochs=10, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy * 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh yes!\n",
      "\tanger   : 7.99%\n",
      "\tdisgust : 1.94%\n",
      "\tfear    : 0.86%\n",
      "\thappy   : 61.29%\n",
      "\tsad     : 2.55%\n",
      "\tshame   : 0.14%\n",
      "\tsurprise: 25.24%\n",
      "I want to cry\n",
      "\tanger   : 6.82%\n",
      "\tdisgust : 2.75%\n",
      "\tfear    : 1.92%\n",
      "\tguilt   : 3.17%\n",
      "\thappy   : 28.03%\n",
      "\tsad     : 53.17%\n",
      "\tshame   : 3.51%\n",
      "\tsurprise: 0.63%\n",
      "What was that?\n",
      "\tanger   : 5.79%\n",
      "\tdisgust : 1.02%\n",
      "\tfear    : 0.55%\n",
      "\thappy   : 20.20%\n",
      "\tsad     : 0.88%\n",
      "\tshame   : 0.10%\n",
      "\tsurprise: 71.46%\n"
     ]
    }
   ],
   "source": [
    "texts = ['Oh yes!', 'I want to cry', 'What was that?']\n",
    "\n",
    "# Predict\n",
    "sequences = t.texts_to_sequences(texts)\n",
    "to_predict = pp.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "prediction = model.predict([to_predict,])\n",
    "\n",
    "# Display prediction results\n",
    "for text in range(len(prediction)):\n",
    "    print(texts[text])\n",
    "    for label in range(len(prediction[text])):\n",
    "        if prediction[text][label] >= .0001:\n",
    "            print(\"\\t{:<{}}: {:.2%}\".format(le.classes_[label], 8, prediction[text][label]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}